services:
  weaviate:
    image: semitechnologies/weaviate:1.27.0
    ports:
      - "8080:8080"
      - "50051:50051"
    environment:
      QUERY_DEFAULTS_LIMIT: 25
      AUTHENTICATION_ANONYMOUS_ACCESS_ENABLED: 'true'
      PERSISTENCE_DATA_PATH: '/var/lib/weaviate'
      DEFAULT_VECTORIZER_MODULE: 'none' 
      ENABLE_MODULES: ''
      GRPC_PORT: '50051'
      CLUSTER_HOSTNAME: 'node1'
      CLUSTER_GOSSIP_BIND_PORT: '7100'
      CLUSTER_DATA_BIND_PORT: '7101'
    healthcheck:
      test: ["CMD", "wget", "--spider", "http://localhost:8080/v1/meta"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s
    volumes:
      - weaviate_data:/var/lib/weaviate

  ollama:
    image: ollama/ollama:latest
    container_name: ollama_container
    ports:
      - "11434:11434"
    volumes:
      - ollama_storage:/root/.ollama
      - ./Modelfiles:/root/Modelfiles
    restart: always
  
  redis-tunnel:
    image: alpine:latest
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 6379 && nc -z localhost 6380"]
      interval: 10s
      timeout: 5s
      retries: 5
    container_name: redis_tunnel
    restart: always
    command: >
      sh -c "apk add --no-cache openssh-client autossh netcat-openbsd &&
      cp /key.pem /root/id_rsa &&
      chmod 600 /root/id_rsa &&
      autossh -M 0 -4 -o StrictHostKeyChecking=no -o ServerAliveInterval=60 -o ServerAliveCountMax=3 -N -L 0.0.0.0:6379:localhost:6379 -L 0.0.0.0:6380:localhost:6380 -i /root/id_rsa ec2-user@${SSH_TARGET_IP}"
    volumes:
      - ~/amazon_rsa_key.pem:/key.pem:ro

  llm_worker_public:
    build: .
    restart: always
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_started
      redis-tunnel:
        condition: service_healthy
    volumes:
      - ./knowledge_base:/app/knowledge_base
    environment:
      REDIS_HOST: "redis-tunnel"
      REDIS_PORT: 6379
      REDIS_QUEUE: "questions:public"
      ENABLE_RAG: "false"
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: "ohp-llama3.2:3b"
      PYTHONUNBUFFERED: 1

  llm_worker_private:
    build: .
    restart: always
    depends_on:
      weaviate:
        condition: service_healthy
      ollama:
        condition: service_started
      redis-tunnel:
        condition: service_healthy
    volumes:
      - ./knowledge_base:/app/knowledge_base
    environment:
      REDIS_HOST: "redis-tunnel"
      REDIS_PORT: 6380
      REDIS_QUEUE: "questions:private"
      ENABLE_RAG: "false"
      WEAVIATE_URL: http://weaviate:8080
      OLLAMA_BASE_URL: http://ollama:11434
      OLLAMA_MODEL: "hbb-llama3.2:3b"
      PYTHONUNBUFFERED: 1

volumes:
  ollama_storage:
  weaviate_data: