services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama_container
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_CONTEXT_LENGTH=4096
      - OLLAMA_DEBUG=false
    volumes:
      - ollama_storage:/root/.ollama
      - ./Modelfiles:/root/Modelfiles
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "1"
  
  kafka-tunnel:
    image: alpine:latest
    healthcheck:
      test: ["CMD-SHELL", "nc -z localhost 9092"]
      interval: 10s
      timeout: 5s
      retries: 5
    container_name: kafka_tunnel
    restart: always
    logging:
      driver: "json-file"
      options:
        max-size: "10m"
        max-file: "1"
    command: >
      sh -c "apk add --no-cache openssh-client autossh netcat-openbsd &&
      cp /key.pem /root/id_rsa &&
      chmod 600 /root/id_rsa &&
      autossh -M 0 -4 -o StrictHostKeyChecking=no -o ServerAliveInterval=60 -o ServerAliveCountMax=3 -N -L 0.0.0.0:9092:127.0.0.1:9092 -i /root/id_rsa ec2-user@${SSH_TARGET_IP}"
    volumes:
      - ~/amazon_rsa_key.pem:/key.pem:ro

  llm_worker_main:
    build: .
    restart: always
    network_mode: service:kafka-tunnel
    depends_on:
      ollama:
        condition: service_started
      kafka-tunnel:
        condition: service_healthy
    environment:
      KAFKA_SERVER: ${KAFKA_SERVER}
      KAFKA_TOPICS: "questions-socius,questions-context-ai,questions-rag"
      OLLAMA_BASE_URL: ${OLLAMA_BASE_URL}
      OLLAMA_MODEL: ${OLLAMA_MODEL}
      WEAVIATE_URL: ${WEAVIATE_URL}
      PYTHONUNBUFFERED: 1

volumes:
  ollama_storage: